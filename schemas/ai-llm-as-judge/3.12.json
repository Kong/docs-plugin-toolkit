{
  "entity_checks": [
    {
      "conditional": {
        "then_field": "config.llm.route_type",
        "then_err": "'config.llm.route_type' must be 'llm/v1/chat' for AI llm as judge plugins",
        "then_match": {
          "eq": "llm/v1/chat"
        },
        "if_match": {
          "not_one_of": [
            "llm/v1/chat"
          ]
        },
        "if_field": "config.llm.route_type"
      }
    },
    {
      "mutually_required": [
        "config.http_proxy_host",
        "config.http_proxy_port"
      ]
    },
    {
      "mutually_required": [
        "config.https_proxy_host",
        "config.https_proxy_port"
      ]
    }
  ],
  "fields": [
    {
      "protocols": {
        "description": "A set of strings representing HTTP protocols.",
        "type": "set",
        "default": [
          "grpc",
          "grpcs",
          "http",
          "https"
        ],
        "elements": {
          "one_of": [
            "grpc",
            "grpcs",
            "http",
            "https"
          ],
          "type": "string"
        },
        "required": true
      }
    },
    {
      "config": {
        "type": "record",
        "fields": [
          {
            "prompt": {
              "description": "Use this prompt to tune the LLM system/assistant message for the llm as a judge prompt.",
              "type": "string",
              "required": true,
              "default": "You are a strict evaluator. You will be given a prompt and a response. Your task is to judge whether the response is correct or incorrect. You must assign a score between 1 and 100, where: 100 represents a completely correct and ideal response, 1 represents a completely incorrect or irrelevant response. Your score must be a single number only â€” no text, labels, or explanations. Use the full range of values (e.g., 13, 47, 86), not just round numbers like 10, 50, or 100. Be accurate and consistent, as this score will be used by another model for learning and evaluation."
            }
          },
          {
            "message_countback": {
              "description": "Number of messages in the chat history to use for evaluating the request",
              "type": "number",
              "default": 1,
              "between": [
                1,
                1000
              ],
              "required": true
            }
          },
          {
            "ignore_system_prompts": {
              "description": "Ignore and discard any system prompts when evaluating the request",
              "type": "boolean",
              "default": true
            }
          },
          {
            "ignore_assistant_prompts": {
              "description": "Ignore and discard any assistant prompts when evaluating the request",
              "type": "boolean",
              "default": true
            }
          },
          {
            "ignore_tool_prompts": {
              "description": "Ignore and discard any tool prompts when evaluating the request",
              "type": "boolean",
              "default": true
            }
          },
          {
            "http_timeout": {
              "description": "Timeout in milliseconds for the AI upstream service.",
              "type": "integer",
              "required": true,
              "default": 60000
            }
          },
          {
            "https_verify": {
              "description": "Verify the TLS certificate of the AI upstream service.",
              "type": "boolean",
              "required": true,
              "default": true
            }
          },
          {
            "sampling_rate": {
              "description": "Judging request sampling rate for configuring the probability-based sampler.",
              "type": "number",
              "default": 1,
              "between": [
                0,
                1
              ],
              "required": false
            }
          },
          {
            "http_proxy_host": {
              "description": "A string representing a host name, such as example.com.",
              "type": "string"
            }
          },
          {
            "http_proxy_port": {
              "description": "An integer representing a port number between 0 and 65535, inclusive.",
              "type": "integer",
              "between": [
                0,
                65535
              ]
            }
          },
          {
            "https_proxy_host": {
              "description": "A string representing a host name, such as example.com.",
              "type": "string"
            }
          },
          {
            "https_proxy_port": {
              "description": "An integer representing a port number between 0 and 65535, inclusive.",
              "type": "integer",
              "between": [
                0,
                65535
              ]
            }
          },
          {
            "llm": {
              "entity_checks": [
                {
                  "conditional": {
                    "then_field": "auth.allow_override",
                    "then_err": "bedrock and gemini only support auth.allow_override = false",
                    "then_match": {
                      "eq": false
                    },
                    "if_match": {
                      "one_of": [
                        "bedrock",
                        "gemini"
                      ]
                    },
                    "if_field": "model.provider"
                  }
                },
                {
                  "mutually_required": [
                    "auth.header_name",
                    "auth.header_value"
                  ]
                },
                {
                  "mutually_required": [
                    "auth.param_name",
                    "auth.param_value",
                    "auth.param_location"
                  ]
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.llama2_format"
                    ],
                    "if_match": {
                      "one_of": [
                        "llama2"
                      ]
                    },
                    "then_err": "must set %s for llama2 provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.mistral_format"
                    ],
                    "if_match": {
                      "one_of": [
                        "mistral"
                      ]
                    },
                    "then_err": "must set %s for mistral provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.anthropic_version"
                    ],
                    "if_match": {
                      "one_of": [
                        "anthropic"
                      ]
                    },
                    "then_err": "must set %s for anthropic provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.azure_instance"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.azure_api_version"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.azure_deployment_id"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider",
                    "if_field": "model.provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "then_at_least_one_of": [
                      "model.options.upstream_url"
                    ],
                    "if_match": {
                      "one_of": [
                        "llama2"
                      ]
                    },
                    "then_err": "must set %s for self-hosted providers/models",
                    "if_field": "model.provider"
                  }
                },
                {
                  "custom_entity_check": {
                    "field_sources": [
                      "route_type",
                      "model",
                      "logging"
                    ]
                  }
                },
                {
                  "custom_entity_check": {
                    "field_sources": [
                      "route_type"
                    ]
                  }
                },
                {
                  "custom_entity_check": {
                    "field_sources": [
                      "route_type",
                      "model"
                    ]
                  }
                },
                {
                  "custom_entity_check": {
                    "field_sources": [
                      "route_type",
                      "model"
                    ]
                  }
                }
              ],
              "type": "record",
              "fields": [
                {
                  "route_type": {
                    "description": "The model's operation implementation, for this provider. ",
                    "type": "string",
                    "one_of": [
                      "llm/v1/chat",
                      "llm/v1/completions",
                      "llm/v1/embeddings",
                      "llm/v1/responses",
                      "llm/v1/assistants",
                      "llm/v1/batches",
                      "llm/v1/files",
                      "image/v1/images/generations",
                      "image/v1/images/edits",
                      "audio/v1/audio/transcriptions",
                      "audio/v1/audio/speech",
                      "audio/v1/audio/translations",
                      "realtime/v1/realtime",
                      "preserve"
                    ],
                    "required": true
                  }
                },
                {
                  "auth": {
                    "type": "record",
                    "fields": [
                      {
                        "header_name": {
                          "description": "If AI model requires authentication via Authorization or API key header, specify its name here.",
                          "type": "string",
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "header_value": {
                          "description": "Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "param_name": {
                          "description": "If AI model requires authentication via query parameter, specify its name here.",
                          "type": "string",
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "param_value": {
                          "description": "Specify the full parameter value for 'param_name'.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "param_location": {
                          "description": "Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.",
                          "type": "string",
                          "one_of": [
                            "query",
                            "body"
                          ],
                          "required": false
                        }
                      },
                      {
                        "azure_use_managed_identity": {
                          "description": "Set true to use the Azure Cloud Managed Identity (or user-assigned identity) to authenticate with Azure-provider models.",
                          "type": "boolean",
                          "required": false,
                          "default": false
                        }
                      },
                      {
                        "azure_client_id": {
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client ID.",
                          "type": "string",
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "azure_client_secret": {
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client secret.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "azure_tenant_id": {
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the tenant ID.",
                          "type": "string",
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "gcp_use_service_account": {
                          "description": "Use service account auth for GCP-based providers and models.",
                          "type": "boolean",
                          "required": false,
                          "default": false
                        }
                      },
                      {
                        "gcp_service_account_json": {
                          "description": "Set this field to the full JSON of the GCP service account to authenticate, if required. If null (and gcp_use_service_account is true), Kong will attempt to read from environment variable `GCP_SERVICE_ACCOUNT`.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "aws_access_key_id": {
                          "description": "Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_ACCESS_KEY_ID environment variable for this plugin instance.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "aws_secret_access_key": {
                          "description": "Set this if you are using an AWS provider (Bedrock) and you are authenticating using static IAM User credentials. Setting this will override the AWS_SECRET_ACCESS_KEY environment variable for this plugin instance.",
                          "type": "string",
                          "encrypted": true,
                          "referenceable": true,
                          "required": false
                        }
                      },
                      {
                        "allow_override": {
                          "description": "If enabled, the authorization header or parameter can be overridden in the request by the value configured in the plugin.",
                          "type": "boolean",
                          "required": false,
                          "default": false
                        }
                      }
                    ],
                    "required": false
                  }
                },
                {
                  "model": {
                    "type": "record",
                    "fields": [
                      {
                        "provider": {
                          "description": "AI provider request format - Kong translates requests to and from the specified backend compatible formats.",
                          "type": "string",
                          "one_of": [
                            "openai",
                            "azure",
                            "anthropic",
                            "cohere",
                            "mistral",
                            "llama2",
                            "gemini",
                            "bedrock",
                            "huggingface"
                          ],
                          "required": true
                        }
                      },
                      {
                        "name": {
                          "description": "Model name to execute.",
                          "type": "string",
                          "required": false
                        }
                      },
                      {
                        "options": {
                          "description": "Key/value settings for the model",
                          "type": "record",
                          "fields": [
                            {
                              "max_tokens": {
                                "description": "Defines the max_tokens, if using chat or completion models.",
                                "type": "integer",
                                "required": false
                              }
                            },
                            {
                              "input_cost": {
                                "description": "Defines the cost per 1M tokens in your prompt.",
                                "type": "number",
                                "gt": 0,
                                "required": false
                              }
                            },
                            {
                              "output_cost": {
                                "description": "Defines the cost per 1M tokens in the output of the AI.",
                                "type": "number",
                                "gt": 0,
                                "required": false
                              }
                            },
                            {
                              "temperature": {
                                "description": "Defines the matching temperature, if using chat or completion models.",
                                "type": "number",
                                "between": [
                                  0,
                                  5
                                ],
                                "required": false
                              }
                            },
                            {
                              "top_p": {
                                "description": "Defines the top-p probability mass, if supported.",
                                "type": "number",
                                "between": [
                                  0,
                                  1
                                ],
                                "required": false
                              }
                            },
                            {
                              "top_k": {
                                "description": "Defines the top-k most likely tokens, if supported.",
                                "type": "integer",
                                "between": [
                                  0,
                                  500
                                ],
                                "required": false
                              }
                            },
                            {
                              "anthropic_version": {
                                "description": "Defines the schema/API version, if using Anthropic provider.",
                                "type": "string",
                                "required": false
                              }
                            },
                            {
                              "azure_instance": {
                                "description": "Instance name for Azure OpenAI hosted models.",
                                "type": "string",
                                "required": false
                              }
                            },
                            {
                              "azure_api_version": {
                                "description": "'api-version' for Azure OpenAI instances.",
                                "type": "string",
                                "required": false,
                                "default": "2023-05-15"
                              }
                            },
                            {
                              "azure_deployment_id": {
                                "description": "Deployment ID for Azure OpenAI instances.",
                                "type": "string",
                                "required": false
                              }
                            },
                            {
                              "llama2_format": {
                                "description": "If using llama2 provider, select the upstream message format.",
                                "type": "string",
                                "one_of": [
                                  "raw",
                                  "openai",
                                  "ollama"
                                ],
                                "required": false
                              }
                            },
                            {
                              "mistral_format": {
                                "description": "If using mistral provider, select the upstream message format.",
                                "type": "string",
                                "one_of": [
                                  "openai",
                                  "ollama"
                                ],
                                "required": false
                              }
                            },
                            {
                              "upstream_url": {
                                "description": "Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.",
                                "type": "string",
                                "required": false
                              }
                            },
                            {
                              "upstream_path": {
                                "description": "Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.",
                                "type": "string",
                                "required": false,
                                "deprecation": {
                                  "message": "llm: config.model.options.upstream_path is deprecated, please use config.model.options.upstream_url instead",
                                  "removal_in_version": "4.0"
                                }
                              }
                            },
                            {
                              "embeddings_dimensions": {
                                "description": "If using embeddings models, set the number of dimensions to generate.",
                                "type": "integer",
                                "gt": 0,
                                "required": false
                              }
                            },
                            {
                              "gemini": {
                                "entity_checks": [
                                  {
                                    "mutually_required": [
                                      "api_endpoint",
                                      "project_id",
                                      "location_id"
                                    ]
                                  }
                                ],
                                "type": "record",
                                "fields": [
                                  {
                                    "api_endpoint": {
                                      "description": "If running Gemini on Vertex, specify the regional API endpoint (hostname only).",
                                      "type": "string",
                                      "required": false
                                    }
                                  },
                                  {
                                    "project_id": {
                                      "description": "If running Gemini on Vertex, specify the project ID.",
                                      "type": "string",
                                      "required": false
                                    }
                                  },
                                  {
                                    "location_id": {
                                      "description": "If running Gemini on Vertex, specify the location ID.",
                                      "type": "string",
                                      "required": false
                                    }
                                  }
                                ],
                                "required": false
                              }
                            },
                            {
                              "bedrock": {
                                "entity_checks": [
                                  {
                                    "mutually_required": [
                                      "aws_assume_role_arn",
                                      "aws_role_session_name"
                                    ]
                                  }
                                ],
                                "type": "record",
                                "fields": [
                                  {
                                    "aws_region": {
                                      "description": "If using AWS providers (Bedrock) you can override the `AWS_REGION` environment variable by setting this option.",
                                      "type": "string",
                                      "required": false
                                    }
                                  },
                                  {
                                    "aws_assume_role_arn": {
                                      "description": "If using AWS providers (Bedrock) you can assume a different role after authentication with the current IAM context is successful.",
                                      "type": "string",
                                      "required": false
                                    }
                                  },
                                  {
                                    "aws_role_session_name": {
                                      "description": "If using AWS providers (Bedrock), set the identifier of the assumed role session.",
                                      "type": "string"
                                    }
                                  },
                                  {
                                    "aws_sts_endpoint_url": {
                                      "description": "If using AWS providers (Bedrock), override the STS endpoint URL when assuming a different role.",
                                      "type": "string"
                                    }
                                  },
                                  {
                                    "embeddings_normalize": {
                                      "description": "If using AWS providers (Bedrock), set to true to normalize the embeddings.",
                                      "type": "boolean",
                                      "default": false
                                    }
                                  },
                                  {
                                    "performance_config_latency": {
                                      "description": "Force the client's performance configuration 'latency' for all requests. Leave empty to let the consumer select the performance configuration.",
                                      "type": "string",
                                      "required": false
                                    }
                                  }
                                ],
                                "required": false
                              }
                            },
                            {
                              "huggingface": {
                                "type": "record",
                                "fields": [
                                  {
                                    "use_cache": {
                                      "description": "Use the cache layer on the inference API",
                                      "type": "boolean",
                                      "required": false
                                    }
                                  },
                                  {
                                    "wait_for_model": {
                                      "description": "Wait for the model if it is not ready",
                                      "type": "boolean",
                                      "required": false
                                    }
                                  }
                                ],
                                "required": false
                              }
                            },
                            {
                              "cohere": {
                                "type": "record",
                                "fields": [
                                  {
                                    "embedding_input_type": {
                                      "description": "The purpose of the input text to calculate embedding vectors.",
                                      "type": "string",
                                      "one_of": [
                                        "search_document",
                                        "search_query",
                                        "classification",
                                        "clustering",
                                        "image"
                                      ],
                                      "default": "classification",
                                      "required": false
                                    }
                                  },
                                  {
                                    "wait_for_model": {
                                      "description": "Wait for the model if it is not ready",
                                      "type": "boolean",
                                      "required": false
                                    }
                                  }
                                ],
                                "required": false
                              }
                            }
                          ],
                          "required": false
                        }
                      }
                    ],
                    "required": true
                  }
                },
                {
                  "logging": {
                    "type": "record",
                    "fields": [
                      {
                        "log_statistics": {
                          "description": "If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.",
                          "type": "boolean",
                          "required": true,
                          "default": false
                        }
                      },
                      {
                        "log_payloads": {
                          "description": "If enabled, will log the request and response body into the Kong log plugin(s) output.",
                          "type": "boolean",
                          "required": true,
                          "default": false
                        }
                      }
                    ],
                    "required": true
                  }
                }
              ],
              "required": true
            }
          }
        ],
        "required": true
      }
    }
  ]
}