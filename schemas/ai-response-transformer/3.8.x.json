{
  "fields": [
    {
      "protocols": {
        "type": "set",
        "required": true,
        "elements": {
          "type": "string",
          "one_of": [
            "grpc",
            "grpcs",
            "http",
            "https"
          ]
        },
        "description": "A set of strings representing HTTP protocols.",
        "default": [
          "grpc",
          "grpcs",
          "http",
          "https"
        ]
      }
    },
    {
      "consumer": {
        "reference": "consumers",
        "type": "foreign",
        "description": "Custom type for representing a foreign key with a null value allowed.",
        "eq": null
      }
    },
    {
      "config": {
        "fields": [
          {
            "prompt": {
              "required": true,
              "type": "string",
              "description": "Use this prompt to tune the LLM system/assistant message for the returning proxy response (from the upstream), adn what response format you are expecting."
            }
          },
          {
            "transformation_extract_pattern": {
              "required": false,
              "type": "string",
              "description": "Defines the regular expression that must match to indicate a successful AI transformation at the response phase. The first match will be set as the returning body. If the AI service's response doesn't match this pattern, a failure is returned to the client."
            }
          },
          {
            "parse_llm_response_json_instructions": {
              "required": true,
              "type": "boolean",
              "description": "Set true to read specific response format from the LLM, and accordingly set the status code / body / headers that proxy back to the client. You need to engineer your LLM prompt to return the correct format, see plugin docs 'Overview' page for usage instructions.",
              "default": false
            }
          },
          {
            "http_timeout": {
              "required": true,
              "description": "Timeout in milliseconds for the AI upstream service.",
              "type": "integer",
              "default": 60000
            }
          },
          {
            "https_verify": {
              "required": true,
              "description": "Verify the TLS certificate of the AI upstream service.",
              "type": "boolean",
              "default": true
            }
          },
          {
            "http_proxy_host": {
              "type": "string",
              "description": "A string representing a host name, such as example.com."
            }
          },
          {
            "http_proxy_port": {
              "type": "integer",
              "description": "An integer representing a port number between 0 and 65535, inclusive.",
              "between": [
                0,
                65535
              ]
            }
          },
          {
            "https_proxy_host": {
              "type": "string",
              "description": "A string representing a host name, such as example.com."
            }
          },
          {
            "https_proxy_port": {
              "type": "integer",
              "description": "An integer representing a port number between 0 and 65535, inclusive.",
              "between": [
                0,
                65535
              ]
            }
          },
          {
            "llm": {
              "fields": [
                {
                  "route_type": {
                    "required": true,
                    "type": "string",
                    "description": "The model's operation implementation, for this provider. Set to `preserve` to pass through without transformation.",
                    "one_of": [
                      "llm/v1/chat",
                      "llm/v1/completions",
                      "preserve"
                    ]
                  }
                },
                {
                  "auth": {
                    "required": false,
                    "type": "record",
                    "fields": [
                      {
                        "header_name": {
                          "required": false,
                          "description": "If AI model requires authentication via Authorization or API key header, specify its name here.",
                          "referenceable": true,
                          "type": "string"
                        }
                      },
                      {
                        "header_value": {
                          "encrypted": true,
                          "referenceable": true,
                          "required": false,
                          "type": "string",
                          "description": "Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'."
                        }
                      },
                      {
                        "param_name": {
                          "required": false,
                          "description": "If AI model requires authentication via query parameter, specify its name here.",
                          "referenceable": true,
                          "type": "string"
                        }
                      },
                      {
                        "param_value": {
                          "encrypted": true,
                          "referenceable": true,
                          "required": false,
                          "type": "string",
                          "description": "Specify the full parameter value for 'param_name'."
                        }
                      },
                      {
                        "param_location": {
                          "required": false,
                          "type": "string",
                          "description": "Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.",
                          "one_of": [
                            "query",
                            "body"
                          ]
                        }
                      },
                      {
                        "azure_use_managed_identity": {
                          "required": false,
                          "type": "boolean",
                          "description": "Set true to use the Azure Cloud Managed Identity (or user-assigned identity) to authenticate with Azure-provider models.",
                          "default": false
                        }
                      },
                      {
                        "azure_client_id": {
                          "required": false,
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client ID.",
                          "referenceable": true,
                          "type": "string"
                        }
                      },
                      {
                        "azure_client_secret": {
                          "encrypted": true,
                          "referenceable": true,
                          "required": false,
                          "type": "string",
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client secret."
                        }
                      },
                      {
                        "azure_tenant_id": {
                          "required": false,
                          "description": "If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the tenant ID.",
                          "referenceable": true,
                          "type": "string"
                        }
                      }
                    ]
                  }
                },
                {
                  "model": {
                    "required": true,
                    "type": "record",
                    "fields": [
                      {
                        "provider": {
                          "required": true,
                          "type": "string",
                          "description": "AI provider request format - Kong translates requests to and from the specified backend compatible formats.",
                          "one_of": [
                            "openai",
                            "azure",
                            "anthropic",
                            "cohere",
                            "mistral",
                            "llama2"
                          ]
                        }
                      },
                      {
                        "name": {
                          "required": false,
                          "type": "string",
                          "description": "Model name to execute."
                        }
                      },
                      {
                        "options": {
                          "required": false,
                          "type": "record",
                          "description": "Key/value settings for the model",
                          "fields": [
                            {
                              "response_streaming": {
                                "default": "allow",
                                "required": false,
                                "type": "string",
                                "description": "Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.",
                                "one_of": [
                                  "allow",
                                  "deny",
                                  "always"
                                ]
                              }
                            },
                            {
                              "max_tokens": {
                                "required": false,
                                "type": "integer",
                                "description": "Defines the max_tokens, if using chat or completion models.",
                                "default": 256
                              }
                            },
                            {
                              "temperature": {
                                "required": false,
                                "type": "number",
                                "description": "Defines the matching temperature, if using chat or completion models.",
                                "between": [
                                  0,
                                  5
                                ]
                              }
                            },
                            {
                              "top_p": {
                                "required": false,
                                "type": "number",
                                "description": "Defines the top-p probability mass, if supported.",
                                "between": [
                                  0,
                                  1
                                ]
                              }
                            },
                            {
                              "top_k": {
                                "required": false,
                                "type": "integer",
                                "description": "Defines the top-k most likely tokens, if supported.",
                                "between": [
                                  0,
                                  500
                                ]
                              }
                            },
                            {
                              "anthropic_version": {
                                "required": false,
                                "type": "string",
                                "description": "Defines the schema/API version, if using Anthropic provider."
                              }
                            },
                            {
                              "azure_instance": {
                                "required": false,
                                "type": "string",
                                "description": "Instance name for Azure OpenAI hosted models."
                              }
                            },
                            {
                              "azure_api_version": {
                                "required": false,
                                "type": "string",
                                "description": "'api-version' for Azure OpenAI instances.",
                                "default": "2023-05-15"
                              }
                            },
                            {
                              "azure_deployment_id": {
                                "required": false,
                                "type": "string",
                                "description": "Deployment ID for Azure OpenAI instances."
                              }
                            },
                            {
                              "llama2_format": {
                                "required": false,
                                "type": "string",
                                "description": "If using llama2 provider, select the upstream message format.",
                                "one_of": [
                                  "raw",
                                  "openai",
                                  "ollama"
                                ]
                              }
                            },
                            {
                              "mistral_format": {
                                "required": false,
                                "type": "string",
                                "description": "If using mistral provider, select the upstream message format.",
                                "one_of": [
                                  "openai",
                                  "ollama"
                                ]
                              }
                            },
                            {
                              "upstream_url": {
                                "required": false,
                                "type": "string",
                                "description": "Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint."
                              }
                            },
                            {
                              "upstream_path": {
                                "required": false,
                                "description": "Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.",
                                "type": "string"
                              }
                            }
                          ]
                        }
                      }
                    ]
                  }
                },
                {
                  "logging": {
                    "required": true,
                    "type": "record",
                    "fields": [
                      {
                        "log_statistics": {
                          "required": true,
                          "type": "boolean",
                          "description": "If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.",
                          "default": false
                        }
                      },
                      {
                        "log_payloads": {
                          "required": true,
                          "type": "boolean",
                          "description": "If enabled, will log the request and response body into the Kong log plugin(s) output.",
                          "default": false
                        }
                      }
                    ]
                  }
                }
              ],
              "entity_checks": [
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "auth.header_name",
                      "auth.param_name"
                    ],
                    "if_match": {
                      "one_of": [
                        "openai",
                        "anthropic",
                        "cohere"
                      ]
                    },
                    "then_err": "must set one of %s, and its respective options, when provider is not self-hosted"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "auth.header_name",
                      "auth.param_name",
                      "auth.azure_use_managed_identity"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set one of %s, and its respective options, when azure provider is set"
                  }
                },
                {
                  "mutually_required": [
                    "auth.header_name",
                    "auth.header_value"
                  ]
                },
                {
                  "mutually_required": [
                    "auth.param_name",
                    "auth.param_value",
                    "auth.param_location"
                  ]
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.llama2_format"
                    ],
                    "if_match": {
                      "one_of": [
                        "llama2"
                      ]
                    },
                    "then_err": "must set %s for llama2 provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.mistral_format"
                    ],
                    "if_match": {
                      "one_of": [
                        "mistral"
                      ]
                    },
                    "then_err": "must set %s for mistral provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.anthropic_version"
                    ],
                    "if_match": {
                      "one_of": [
                        "anthropic"
                      ]
                    },
                    "then_err": "must set %s for anthropic provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.azure_instance"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.azure_api_version"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.azure_deployment_id"
                    ],
                    "if_match": {
                      "one_of": [
                        "azure"
                      ]
                    },
                    "then_err": "must set %s for azure provider"
                  }
                },
                {
                  "conditional_at_least_one_of": {
                    "if_field": "model.provider",
                    "then_at_least_one_of": [
                      "model.options.upstream_url"
                    ],
                    "if_match": {
                      "one_of": [
                        "mistral",
                        "llama2"
                      ]
                    },
                    "then_err": "must set %s for self-hosted providers/models"
                  }
                },
                {
                  "custom_entity_check": {
                    "field_sources": [
                      "route_type",
                      "model",
                      "logging"
                    ]
                  }
                }
              ],
              "type": "record",
              "required": true
            }
          }
        ],
        "type": "record",
        "required": true
      }
    }
  ],
  "entity_checks": [
    {
      "conditional": {
        "if_field": "config.llm.route_type",
        "then_err": "'config.llm.route_type' must be 'llm/v1/chat' for AI transformer plugins",
        "if_match": {
          "not_one_of": [
            "llm/v1/chat"
          ]
        },
        "then_field": "config.llm.route_type",
        "then_match": {
          "eq": "llm/v1/chat"
        }
      }
    },
    {
      "mutually_required": [
        "config.http_proxy_host",
        "config.http_proxy_port"
      ]
    },
    {
      "mutually_required": [
        "config.https_proxy_host",
        "config.https_proxy_port"
      ]
    }
  ]
}