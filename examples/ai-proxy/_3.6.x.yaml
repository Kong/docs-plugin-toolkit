name: ai-proxy
config:
  route_type: "llm/v1/chat"
  model:
  - name: "openai"
    provider: "openai"
    options:
      max_tokens: 256
      temperature: 1.0
      upstream_url: "http://example"